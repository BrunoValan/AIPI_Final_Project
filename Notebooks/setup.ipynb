{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import OrdinalEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV, KFold\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(): \n",
    "    \n",
    "    '''This function loads the and merges data, drops irrelevant features, handles missingness and outliers and performs feature engineering\n",
    "    Inputs: \n",
    "        none \n",
    "    Returns: \n",
    "        df_prepped (pd.DataFrame): data frame with cleaned patient and geographic data\n",
    "\n",
    "    '''\n",
    "\n",
    "    #loads patient data\n",
    "    patient_data = pd.read_csv('../Data_Clean/RTED_ADIMERGE.csv')\n",
    "    geographic_data = pd.read_csv('../Data_Clean/geographic_data_clean.csv')\n",
    "\n",
    "    #drop state since it is already in the patient data frame\n",
    "    geographic_data = geographic_data.drop(labels = ['STATE'], axis = 1)\n",
    "\n",
    "    #encode ZIP_5 as a object for the merge on geographic data frame \n",
    "    geographic_data['ZIP_5'] = geographic_data['ZIP_5'].astype(str)\n",
    "\n",
    "    #encode ZIP_5 as a object for merge on patient data frame\n",
    "    patient_data['ZIP_5'] = patient_data['ZIP_5'].astype(str)\n",
    "\n",
    "    #still some float values in there which we are causing data loss on the merge - grab first 5 characters of the string\n",
    "    patient_data['ZIP_5'] = patient_data['ZIP_5'].str[:5]\n",
    "\n",
    "    #merge patient and geographic data frame\n",
    "    df_full = pd.merge(patient_data, geographic_data, how = 'left', on = 'ZIP_5')\n",
    "\n",
    "    #cols to drop due to leakge or redundancy \n",
    "    cols_drop = ['RET_CSN', 'RET_DAYS', 'RET_HOSPITAL', 'RET_ED_DISPO', 'RET_CHIEF_COMPLAINT', \n",
    "                'RET_CLINICAL_IMPRESSION', 'RET_HB_PRIM_DX_CODE', 'RET_HB_PRIM_DX_NAME', 'RET_ED_DENOM',\n",
    "                'RET_ED30_NUMER', 'EDRevisitDischargedPatient', 'READMISSION', 'Readmission90', 'EDRevisit90', \n",
    "                'PAT_ZIP', 'COUNTY', 'CITY', 'location', 'Latitude', 'Longitude', 'WEIGHTED_ADI', \n",
    "                'PAYOR_NAME', 'ATTENDING_PROV', 'OR_LOGS', 'OR_LOG_ROW', 'LOCATION_NAME', 'LOCATION_NM', \n",
    "                'SERVICE_NAME', 'PRIMARY_PHYSICIAN_NM', 'CLIN_DEP', 'PRIMARY_PROCEDURE_NM', 'PRIMARY_PROCEDURE_CPT']\n",
    "    \n",
    "    #drop na for columns with minimal na values\n",
    "    cols_dropna = ['RACE', 'ETHNIC_GROUP', 'DX_HYPERTENSION', 'DX_RENAL_FAILURE', 'DX_COPD', 'DX_TYPE_2_DM', 'DX_HIP_FRACTURE', 'DX_OSTEOPOROSIS', 'STATE', 'PRIMARY_PROC_CPT_CODE']\n",
    "\n",
    "    #we will fill with median imputation for these columns\n",
    "    cols_fillna = ['BMI', 'distance_to_hospital']\n",
    "    \n",
    "    #drops columns\n",
    "    df_simplified = df_full.drop(labels = cols_drop, axis = 1)\n",
    "\n",
    "    #drop na for columns with only a few missing values \n",
    "    df_clean = df_simplified.dropna(subset = cols_dropna)\n",
    "\n",
    "    #fill BMI and distance with the median due to right tailed distribution (will handle outliers later)  \n",
    "    df_clean[cols_fillna] = df_clean[cols_fillna].fillna(df_clean[cols_fillna].median())\n",
    "\n",
    "    #re-assign to clean_df\n",
    "    clean_df = df_clean.copy()\n",
    "\n",
    "    #rename response columns \n",
    "    clean_df['RETURN_ED_90DAY'] = clean_df['ED90Day']\n",
    "\n",
    "    #drop original\n",
    "    clean_df = clean_df.drop(labels = ['ED90Day'], axis = 1)\n",
    "\n",
    "    #creates list of BMI anomolies to drop \n",
    "    k = 3\n",
    "    col = clean_df['BMI']\n",
    "    col_std = np.std(clean_df['BMI'])\n",
    "    col_mean = np.mean(clean_df['BMI'])\n",
    "    thresh = col_std * k\n",
    "    lower_limit  = col_mean - thresh\n",
    "    upper_limit = col_mean + thresh\n",
    "    BMI_anomalies = list(col.index[(col>upper_limit) | (col<lower_limit)])\n",
    "\n",
    "    #drops BMI outliers\n",
    "    df_no_outliers = clean_df.drop(BMI_anomalies, axis =0)\n",
    "\n",
    "    #drops instances when LOS is >90 days \n",
    "    df_no_outliers = df_no_outliers[df_no_outliers['LOS_DAYS'] <= 90]\n",
    "\n",
    "    #next we will create binary flag variables for minority status and ethnic minority status\n",
    "    minority_races = ['Black or African American', 'Other', 'Asian', 'American Indian or Alaskan Native', 'Native Hawaiian or Other Pacific Islander']\n",
    "    ethnic_minority = ['Hispanic Other', 'Hispanic Mexican', 'Hispanic Puerto Rican', 'Hispanic Cuban']\n",
    "\n",
    "    #creates racial minority column \n",
    "    df_no_outliers['RacialMinority'] = np.where(df_no_outliers['RACE'].isin(minority_races), 1,0)\n",
    "\n",
    "    #creates ethnic minority column \n",
    "    df_no_outliers['EthnicMinority'] = np.where(df_no_outliers['ETHNIC_GROUP'].isin(ethnic_minority), 1,0)\n",
    "\n",
    "    #creates language not english column \n",
    "    df_no_outliers['LanguageNotEnglish'] = np.where(df_no_outliers['PAT_LANGUAGE'] == 'English', 0,1)\n",
    "\n",
    "    df_modelling = df_no_outliers.copy()\n",
    "\n",
    "    #columns deemed unimportant from univariate testing\n",
    "    cols_unimportant = ['PAT_CLASS', 'BMI', 'PAT_LANGUAGE', 'ETHNIC_GROUP', 'FINANCIAL_CLASS_NAME', \n",
    "             'RACE', 'ZIP_5', 'DISCH_DEPT', 'LOCATION_ID', 'ZIP_5',\n",
    "             'WhiteNonHipanic', 'Race_OtherAsian', 'Race_Other', 'Race_NotReported', 'Race_NativeHawaiian',\n",
    "             'Race_White', 'Race_Black', 'Race_Asian', 'Race_AmericanIndian', 'Race_NotValid', \n",
    "             'Hispanic_7', 'Hispanic_NotHispanic', 'Hispanic_5', 'Hispanic_4', 'Hispanic_3', \n",
    "             'Hispanic_2', 'Hispanic_1', 'RaceDummy_5', 'RaceDummy_4', 'FinancialClass_Commercial',\n",
    "             'FinancialClass_Liability', 'FinancialClass_ManagedCare','FinancialClass_MedicaidPending', \n",
    "             'FinancialClass_MediCARE','FinancialClass_MedicareAdvantage','FinancialClass_CommercialBlueCross', \n",
    "             'FinancialClass_MedicaidNC','FinancialClass_MedicaidManaged', 'FinancialClass_CommercialBCOOS',\n",
    "            'FinancialClass_Medcaid', 'FinancialClass_12Unkonwn','FinancialClass_13', 'FinancialClass_14', \n",
    "            'FinancialClass_15','FinancialClass_WorkersComp', 'Sex_Female', 'Sex_2', 'Sex_3', 'PRIMARY_PROC_CPT_CODE']\n",
    "    \n",
    "    #drop unimportant columns\n",
    "    df_prepped = df_modelling.drop(labels = cols_unimportant, axis = 1)\n",
    "\n",
    "    return df_prepped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepped = prep_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_feats(df_prepped): \n",
    "    '''Encodes the remaining catagorical features using one hot encoding, then filters for only the relevant features. Splits data into training and test sets\n",
    "\n",
    "    Inputs: \n",
    "        df_prepped (pd.DataFrame): cleaned data set from prep_data()\n",
    "\n",
    "    Returns: \n",
    "        X_train (pd.DataFrame): encoded training set \n",
    "        X_test (pd.DataFrame): encoded test set \n",
    "        y_train (numpy array): training labels \n",
    "        y_test (numpy array): test labels\n",
    "    '''\n",
    "    #cols to encode \n",
    "    cols_onehot = ['SEX', 'DISCH_LOC_ABBR', 'DISCHARGE_DISPO', 'CASE_CLASS_NM', 'CLIN_DIV', 'PAT_BASE_CLASS', 'STATE']\n",
    "\n",
    "    #Make sure all categorical columns are string type\n",
    "    for col in cols_onehot:\n",
    "        df_prepped[col] = df_prepped[col].astype(str)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    onehot_enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    # Fit encoder on training data\n",
    "    onehot_enc.fit(df_prepped[cols_onehot])\n",
    "    # Get the names of the new columns created\n",
    "    colnames = columns=list(onehot_enc.get_feature_names_out(input_features=cols_onehot))\n",
    "    # Transform the data\n",
    "    onehot_vals = onehot_enc.transform(df_prepped[cols_onehot]).toarray()\n",
    "    # Put transformed data into dataframe.  Make sure index matches\n",
    "    enc_df = pd.DataFrame(onehot_vals,columns=colnames,index=df_prepped.index)\n",
    "    # Add onehot columns back onto original dataframe and drop the original columns\n",
    "    encoded_df = pd.concat([df_prepped,enc_df],axis=1).drop(cols_onehot,axis=1)\n",
    "\n",
    "    #list of top 20 important features\n",
    "    top_20 = ['distance_to_hospital', 'AGE', 'LOS_DAYS', 'SevereObesity', 'RacialMinority', \n",
    "              'DX_HYPERTENSION', 'DX_TYPE_2_DM', 'SEX_Female', 'Elderly65', 'SEX_Male', 'DISCH_LOC_ABBR_DUH', \n",
    "              'CLIN_DIV_TOTAL JOINT', 'MedicaidBinary', 'DX_COPD', 'DX_RENAL_FAILURE', 'DISCH_LOC_ABBR_DRAH', \n",
    "              'DISCH_LOC_ABBR_DRH', 'CLIN_DIV_HAND', 'PAT_BASE_CLASS_Inpatient', 'DISCHARGE_DISPO_Home or Self Care', 'RETURN_ED_90DAY']\n",
    "    \n",
    "    final_df = encoded_df[top_20]\n",
    "\n",
    "    return final_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = encode_feats(df_prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(final_df): \n",
    "    '''\n",
    "    Funciton splits final data set into training and test set, trains optimized random forest and makes prediction on test set. \n",
    "    It also saves the trained model object into the model directory and prints the classification report for the predictions \n",
    "    on the test set. \n",
    "\n",
    "    Inputs: final_df (pd.DataFrame): data frame of the cleaned and encoded data set with only the relevant features we will use in our final model\n",
    "\n",
    "    Returns: final model object (sklearn.ensemble._forest.RandomForestClassifier)\n",
    "    '''\n",
    "\n",
    "    # Create feature matrix\n",
    "    X = final_df.drop(labels='RETURN_ED_90DAY',axis=1)\n",
    "\n",
    "    # Create target vector\n",
    "    y = final_df['RETURN_ED_90DAY'].copy().to_numpy()\n",
    "\n",
    "    # Split our data\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=0.2, stratify= y)\n",
    "\n",
    "    #define final model\n",
    "    final_model = RandomForestClassifier(criterion = 'gini', max_features= 0.1, max_depth = None, \n",
    "                                 max_samples= 0.3, min_samples_leaf= 1, n_estimators= 1000, random_state = 0)\n",
    "    #fit model to training & test data\n",
    "    final_model.fit(X_train,y_train)\n",
    "\n",
    "    #get final preditions\n",
    "    final_preds = final_model.predict(X_test)\n",
    "\n",
    "    #get probability from the model\n",
    "    predict_proba = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    #change prediction threshold 0.2 \n",
    "    preds_custom = (predict_proba >= 0.2).astype(int)\n",
    "\n",
    "    #saves classification report as an object w/ lower prediction threshold \n",
    "    report_custom = classification_report(y_test, preds_custom)\n",
    "\n",
    "    #saves classification report as an object w/ lower default threshold \n",
    "    report_default = classification_report(y_test, final_preds)\n",
    "    \n",
    "    #prints both classification reports\n",
    "    print(f'the classification report for the optimal RF on TEST set (prediction threshold = 0.2) \\n {report_custom}')\n",
    "    print(f'the classification report for the optimal RF on the TEST set (prediction threshold = 0.5) is\\n {report_default}')\n",
    "\n",
    "    dump(final_model, '../models/optimized_random_forest.pkl')\n",
    "\n",
    "    return final_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the classification report for the optimal RF on TEST set (prediction threshold = 0.2) \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.91      0.93      4880\n",
      "         1.0       0.27      0.38      0.31       407\n",
      "\n",
      "    accuracy                           0.87      5287\n",
      "   macro avg       0.61      0.65      0.62      5287\n",
      "weighted avg       0.89      0.87      0.88      5287\n",
      "\n",
      "the classification report for the optimal RF on the TEST set (prediction threshold = 0.5) is\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      1.00      0.96      4880\n",
      "         1.0       0.50      0.02      0.03       407\n",
      "\n",
      "    accuracy                           0.92      5287\n",
      "   macro avg       0.71      0.51      0.50      5287\n",
      "weighted avg       0.89      0.92      0.89      5287\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = modelling(final_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPI510",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
